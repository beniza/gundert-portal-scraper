{
  "project_overview": {
    "name": "Gundert Portal Scraper - Text Extraction System",
    "primary_goals": [
      "Extract book content from Gundert Portal/OpenDigi digital manuscript collections",
      "Transform extracted content to multiple output formats (USFM, TEI XML, BibleML, ParaBible JSON, DOCX) for various academic and publishing use cases and for Bible translation projects", 
      "Provide production-ready CLI tool with validation and quality assurance"
    ],
    "key_challenge_solved": "The Gundert Portal content is stored as Single Page Applications (SPAs). So make a call to a page and download the content. Then perform the post downloaded operation on the downloaded content",
    "example_target_url": "https://opendigi.ub.uni-tuebingen.de/opendigi/GaXXXIV5a",
    "example_target_content": "Gundert's 1881 Malayalam Psalms manuscript"
  },

  "architectural_learnings": {
    "two_phase_extraction_architecture": {
      "rationale": "SPA websites require different extraction strategy than traditional multi-page sites",
      "cache_safety_critical": "IMPORTANT: Cache files in ./cache/ must NEVER be automatically deleted. They contain downloaded manuscript content that should persist across all operations. Only explicit user commands should clear cache.",
      "components": {
        "download_phase": {
          "purpose": "Single navigation to SPA website, cache all page structures and metadata",
          "implementation": "Selenium WebDriver with JavaScript execution to load dynamic content",
          "cache_strategy": "Save complete HTML page source to JSON file with metadata timestamp",
          "cache_protection": "Check if cached before downloading. Use force_redownload=False by default to preserve cached content"
        },
        "processing_phase": {
          "purpose": "Extract real content from cached SPA DOM without browser",
          "key_insight": "All content is embedded in TEI XML within #transcript-content div. Use BeautifulSoup to parse cached HTML",
          "implementation": "Parse TEI <surface n='X'> elements for each page, extract text nodes while preserving line structure",
          "performance": "Once cached, can extract any page range instantly without Selenium (10-30x faster)",
          "cache_preservation": "Processing phase NEVER modifies or deletes cache files"
        },
        "cache_management_rules": {
          "never_auto_delete": "Cache files must never be deleted by extract, transform, or cleanup operations",
          "gitignore_required": "cache/ directory must be in .gitignore to prevent git commits",
          "force_flag_required": "Re-downloading requires explicit --force-redownload flag",
          "backup_recommended": "Users should backup cache/ directory locally (not in git)",
          "typical_size": "771KB for 201-page manuscript - minimal storage overhead"
        }
      }
    },
    
    "json_output_structure": {
      "purpose": "Structured intermediate format preserving all extracted data",
      "schema": {
        "metadata": {
          "book_id": "Extracted from URL (e.g., GaXXXIV5a)",
          "url": "Original OpenDigi URL",
          "title": "Book title extracted from page or book_id",
          "content_type": "Detected type: bible, dictionary, book, manuscript",
          "language": "Primary language (e.g., malayalam)",
          "script": "Writing script (e.g., malayalam, latin)",
          "total_pages": "Number of pages in manuscript",
          "extraction_date": "ISO timestamp",
          "extractor_version": "Version of scraper",
          "notes": "Any extraction notes or warnings",
          "extra_metadata": "Additional metadata dictionary"
        },
        "pages": [
          {
            "page_number": "Sequential page number",
            "image_url": "URL to scanned page image (critical for page-text alignment)",
            "lines": "Array of text lines preserving line breaks from transcript",
            "full_text": "Complete page text with newlines",
            "has_heading": "Boolean - detected heading on page",
            "has_verse_numbers": "Boolean - detected verse numbering",
            "confidence": "Float 0-1 extraction confidence score",
            "notes": "Array of extraction notes/warnings"
          }
        ],
        "statistics": {
          "total_lines_extracted": "Total lines across all pages",
          "total_characters": "Total character count",
          "pages_with_content": "Number of pages with non-empty content",
          "extraction_errors": "Number of errors encountered",
          "success_rate": "Percentage of successful extractions"
        }
      },
      "design_rationale": "This structure enables: (1) Page-to-image mapping for verification, (2) Line-level preservation for accurate formatting, (3) Metadata for downstream transformations, (4) Quality metrics for validation"
    },

    "spa_content_extraction_strategy": {
      "task": "JavaScript-based extraction within loaded SPA context",
      "critical_code_pattern": "driver.execute_script() to manipulate SPA navigation and extract real content",
      "css_selectors": [
        ".transcript-text",
        ".info-content",
        ".page-content"
      ]
    },

    "format_transformation_pipeline": {
      "plugin_architecture": "BaseTransformer with format-specific plugins",
      "successful_formats": [
        "TEI XML",
        "DOCX",
        "ParaBible JSON",
		"USFM"
      ]
    }
  },

  "technical_stack": {
    "core_technologies": {
      "python": "3.10+",
      "selenium": "4.27.1+ (essential for SPA JavaScript execution)",
      "webdriver_manager": "4.0.2+ (Chrome/Chromium automation)",
      "beautifulsoup4": "HTML parsing (secondary to Selenium for SPAs)",
      "lxml": "XML processing and document generation",
      "click": "8.3.0+ CLI framework",
      "rich": "14.2.0+ enhanced console output",
      "requests": "HTTP client (limited use due to SPA architecture)"
    },

    "specialized_libraries": {
      "usfm_grammar": "3.1.2+ USFM validation",
      "python_docx": "1.1.2+ Word document generation", 
      "jsonschema": "4.23.0+ data validation",
      "pytest": "8.3.5+ testing framework for comprehensive test coverage"
    }
  },

  "project_structure": {
    "src/gundert_portal_scraper/": {
      "core/": {
        "purpose": "Book identification, connection management, caching infrastructure",
        "key_files": [
          "download_phase.py - SPA-optimized single download with performance metrics",
          "processing_phase.py - JavaScript-based content extraction from cached SPA DOM", 
          "gundert_scraper.py - Orchestrates download → processing workflow",
          "cache.py - RawContentCache for book-level caching with metadata"
        ]
      },
      "extraction/": {
        "purpose": "Content scraping, metadata extraction, text processing from the cached json",
        "key_files": [
          "content_scraper.py - Page-by-page content extraction with line-level preservation",
          "metadata.py - Content type detection (biblical, linguistic, literary)",
          "text_processor.py - Text cleaning and Unicode handling"
        ]
      },
      "transformations/plugins/": {
        "purpose": "Format-specific transformation plugins",
        "transformers": [
          "tei_transformer.py - TEI XML with proper manuscript structure and metadata",
          "docx_transformer.py - Microsoft Word with professional styling and formatting",
          "parabible_transformer.py - Structured JSON format for API consumption",
          "usfm_transformer.py - Unified Standard Format Markers for biblical content",
          "bibleml_transformer.py - BibleML/OSIS format for scripture applications"
        ]
      },
      "storage/": {
        "purpose": "Data schemas, persistence, format conversion",
        "schemas.py": "BookStorage, PageContent, BookMetadata Pydantic models"
      },
      "validation/": {
        "purpose": "Content quality assurance and format validation",
        "validators/": "Format-specific validation (USFM, TEI, etc.)"
      },
      "cli/": {
        "purpose": "Command-line interface with rich console output",
        "commands.py": "Extract, transform, validate, batch processing commands"
      }
    }
  },

  "critical_implementation_steps": {
    "1_project_setup": {
      "create_directory_structure": "Follow src/gundert_portal_scraper/ modular layout",
      "setup_pyproject_toml": {
        "dependencies": "selenium, webdriver-manager, click, rich, lxml, python-docx, beautifulsoup4, pydantic",
        "dev_dependencies": "pytest, pytest-cov, responses, selenium",
        "entry_point": "gundert-scraper = gundert_portal_scraper.cli:cli"
      },
      "install_chrome": "Required for Selenium WebDriver automation"
    },

    "2_core_spa_extraction": {
      "implement_cache_system": {
        "file": "src/gundert_portal_scraper/core/cache.py",
        "class": "RawContentCache",
        "critical_rule": "CACHE MUST NEVER BE AUTO-DELETED - Cache contains downloaded manuscripts and should persist across all operations",
        "methods": [
          "save(book_id, content, metadata) - Save HTML to JSON cache",
          "load(book_id) - Load cached content",
          "is_cached(book_id) - Check if book is cached",
          "clear(book_id) - ONLY for explicit user command, never auto-called"
        ],
        "cache_format": "JSON file with content, metadata, cached_at timestamp",
        "safety_requirements": [
          "Check cache before any download operation",
          "Never delete cache files automatically",
          "Require force_redownload flag to override cache",
          "Add cache/ to .gitignore",
          "Never call clear() from extract/transform operations"
        ]
      },
      "implement_two_phase_scraper": {
        "file": "src/gundert_portal_scraper/extraction/two_phase_scraper.py",
        "class": "TwoPhaseContentScraper",
        "cache_safety": "MUST check cache first. Only download if not cached OR force_redownload=True",
        "download_phase_logic": [
          "1. Check if book_id is cached, return cached if available (UNLESS force_redownload=True)",
          "2. If not cached: Connect Selenium to OpenDigi URL",
          "3. Wait for page load (3-5 seconds for SPA initialization)",
          "4. Extract driver.page_source (contains full embedded TEI XML)",
          "5. Save to cache with metadata",
          "6. Return cached content",
          "IMPORTANT: Cache is preserved - never deleted by this operation"
        ],
        "processing_phase_logic": [
          "1. Parse cached HTML with BeautifulSoup",
          "2. Find div#transcript-content containing TEI XML",
          "3. Find all <surface n='X'> elements (each = one page)",
          "4. For each requested page number:",
          "   a. Find surface with matching n attribute",
          "   b. Extract all text nodes (preserving structure)",
          "   c. Clean and split into lines",
          "   d. Detect verse numbers and headings",
          "   e. Create PageContent object",
          "5. Return BookStorage with all pages"
        ]
      },
      "key_opendigi_structure": {
        "html_structure": "<div id='transcript-content'><tei><sourcedoc><surface n='1'>...</surface><surface n='2'>...</surface>...",
        "page_identification": "Each <surface n='X'> element represents one manuscript page",
        "text_extraction": "Use TreeWalker or descendants to get all text nodes",
        "critical_insight": "All 200+ pages are embedded in single HTML response - no need for page-by-page requests"
      }
    },

    "3_data_models": {
      "pydantic_schemas": {
        "file": "src/gundert_portal_scraper/storage/schemas.py",
        "models": [
          "BookMetadata - book_id, url, title, content_type, language, script, total_pages, etc.",
          "PageContent - page_number, image_url, lines (list), full_text, has_heading, has_verse_numbers, confidence, notes",
          "BookStorage - metadata (BookMetadata), pages (list[PageContent]), statistics (dict)"
        ],
        "methods": [
          "BookStorage.to_json(filepath) - Save to JSON file",
          "BookStorage.from_json(filepath) - Load from JSON file",
          "BookStorage.update_statistics() - Calculate statistics from pages"
        ]
      }
    },

    "4_content_transformation": {
      "base_transformer_framework": {
        "abstract_methods": "transform(), is_compatible(), validate_input()",
        "line_mapping": "(Optional) Track source → output line relationships. The transcript contains a line level mapping (seperated by soft enter) with the image",
        "error_handling": "Graceful degradation with detailed error reporting"
      },
      "tei_xml_transformer": {
        "tei_header": "Proper TEI metadata with language identification",
        "manuscript_structure": "Page breaks, paragraphs, content structure preservation",
        "content_preservation": "Unicode handling, historical typography, script-specific formatting"
      }
    },

    "4_cli_and_validation": {
      "cli_implementation": {
        "extract_command": "URL → formats → output path workflow",
        "transform_command": "BookStorage → multiple output formats",
        "validate_command": "Format-specific content validation",
        "batch_processing": "Multiple books with parallel processing"
      },
      "validation_framework": {
        "format_validators": "USFM, TEI XML, ParaBible JSON validation",
        "content_specific": "Unicode validation, encoding checks, script-specific patterns",
        "quality_metrics": "Success rates, content completeness, extraction accuracy"
      }
    },

    "5_testing_and_documentation": {
      "test_coverage": "Implement unit and integration tests",
      "test_categories": [
        "SPA extraction accuracy",
        "Format transformation correctness", 
        "Vernacular content preservation",
        "CLI command functionality"
      ],
      "documentation": [
        "README.md - Project overview and quick start",
        "USER_GUIDE.md - Complete CLI usage",
        "DEVELOPER_GUIDE.md - Architecture and API",
        "API_REFERENCE.md - Programmatic interface"
      ]
    }
  },

  "known_issues_and_solutions": {
    "spa_content_duplication": {
      "problem": "If we send request to download for each page, scraping returns identical content for all pages",
      "solution": "Two-phase architecture with JavaScript-based extraction",
      "prevention": "Always test SPA detection and implement fallback strategies"
    }
  },

  "success_metrics": {
    "content_extraction": {
      "clean_text_extraction": "Extract the full text content while filtering out page numbers, headers, and navigation elements",
      "performance_optimization": "Two-phase approach (Download + Processing) should provide measurable performance benefits over page-by-page extraction",
      "content_accuracy": "Verify unique content per page (no duplicate content across pages)",
      "structural_preservation": "Maintain document hierarchy, paragraphs, and content organization"
    },
    "format_transformation": {
      "multi_format_success": "Generate valid output in multiple formats (TEI XML, DOCX, JSON)",
      "format_compliance": "Each output format should meet its respective standards and schemas",
      "validation_passing": "All generated content should pass format-specific validation",
      "metadata_preservation": "Source information, page references, and document structure maintained"
    },
    "software_quality": {
      "modular_architecture": "Plugin-based transformation system with clear separation of concerns", 
      "test_coverage": "Comprehensive test suite with high coverage (target 80%+)",
      "cli_interface": "Professional command-line tool with progress indicators and rich console output",
      "error_handling": "Graceful degradation and informative error messages",
      "documentation": "Complete user and developer documentation"
    }
  },

  "recreate_from_scratch_checklist": [
    {
      "step": "Environment Setup",
      "actions": [
        "Create Python 3.10+ virtual environment",
        "Install Chrome/Chromium browser",
        "Set up pyproject.toml with exact dependencies",
        "Initialize git repository with proper .gitignore"
      ]
    },
    {
      "step": "Core Infrastructure",
      "actions": [
        "Implement modular directory structure",
        "Create Pydantic schemas (BookStorage, PageContent)",
        "Build caching system with RawContentCache",
        "Set up logging and configuration management"
      ]
    },
    {
      "step": "SPA Extraction Engine", 
      "actions": [
        "Implement DownloadPhase with Selenium integration",
        "Build ProcessingPhase with JavaScript execution",
        "Create TwoPhaseContentScraper orchestrator",
        "Add SPA detection and fallback mechanisms"
      ]
    },
    {
      "step": "Transformation Pipeline",
      "actions": [
        "Build BaseTransformer abstract class",
        "Implement TEI XML transformer (working reference)",
        "Create DOCX transformer with styling",
        "Debug and fix USFM transformer compatibility"
      ]
    },
    {
      "step": "CLI and Validation",
      "actions": [
        "Build Click-based CLI with rich console output", 
        "Implement extract/transform/validate commands",
        "Create format-specific validators",
        "Add batch processing capabilities"
      ]
    },
    {
      "step": "Quality Assurance",
      "actions": [
        "Write comprehensive test suite (target 88% coverage)",
        "Create integration tests with real URLs",
        "Add performance benchmarks",
        "Document all APIs and usage patterns"
      ]
    }
  ],
  
  "llm_recreation_instructions": {
    "purpose": "Complete instructions for an LLM to recreate this scraper from scratch",
    "prerequisites": [
      "Python 3.10+ environment",
      "Chrome/Chromium browser installed",
      "Basic understanding of Selenium and BeautifulSoup"
    ],
    "step_by_step_guide": {
      "step_1_analyze_target": {
        "instruction": "Navigate to https://opendigi.ub.uni-tuebingen.de/opendigi/GaXXXIV5a in browser DevTools",
        "what_to_find": [
          "Open DevTools → Elements → Find <div id='transcript-content'>",
          "Notice it contains <tei><sourcedoc> with multiple <surface n='X'> elements",
          "Each surface represents one page, identified by n attribute",
          "All 200+ pages are embedded in the initial HTML (SPA architecture)",
          "Conclusion: Download once, parse locally - no need for page-by-page requests"
        ]
      },
      "step_2_setup_project": {
        "instruction": "Create Python project with modular structure",
        "directory_structure": [
          "src/gundert_portal_scraper/",
          "  core/ - cache.py, book_identifier.py, connector.py",
          "  extraction/ - two_phase_scraper.py",
          "  storage/ - schemas.py",
          "  cli/ - commands.py",
          "cache/ - for cached content",
          "output/ - for extracted data"
        ],
        "dependencies": "selenium, webdriver-manager, beautifulsoup4, pydantic, click, rich, lxml"
      },
      "step_3_implement_cache": {
        "file": "core/cache.py",
        "class": "RawContentCache",
        "purpose": "Save downloaded HTML to avoid repeated browser connections",
        "methods": {
          "save": "Save HTML string + metadata to JSON file named {book_id}_content.json",
          "load": "Load and return cached content dictionary",
          "is_cached": "Check if cache file exists",
          "get_cache_path": "Return Path object for cache file"
        }
      },
      "step_4_implement_connector": {
        "file": "core/connector.py",
        "class": "GundertPortalConnector",
        "purpose": "Manage Selenium WebDriver connection",
        "key_code": [
          "Use webdriver_manager.chrome.ChromeDriverManager for automatic driver setup",
          "Configure headless mode, no-sandbox, disable-dev-shm-usage",
          "navigate_to_book(page_num): Load URL with optional page parameter",
          "get_page_source(): Return driver.page_source containing all embedded content",
          "execute_script(script): Execute JavaScript if needed",
          "close(): Cleanup driver"
        ]
      },
      "step_5_implement_schemas": {
        "file": "storage/schemas.py",
        "models": [
          "BookMetadata: book_id, url, title, content_type, language, script, total_pages, extraction_date, extractor_version",
          "PageContent: page_number, image_url (CRITICAL!), lines (list[str]), full_text, has_heading, has_verse_numbers, confidence, notes",
          "BookStorage: metadata (BookMetadata), pages (list[PageContent]), statistics (dict) with methods to_json() and from_json()"
        ],
        "image_url_field": {
          "field_name": "image_url",
          "field_type": "Optional[str]",
          "purpose": "Links extracted text to source manuscript image for verification and citation",
          "example": "https://opendigi.ub.uni-tuebingen.de/opendigi/image/GaXXXIV5a/GaXXXIV5a_007.jp2/full/full/0/default.jpg",
          "critical_note": "This field enables page-to-image alignment which is essential for academic work and quality validation"
        }
      },
      "step_6_implement_two_phase_scraper": {
        "file": "extraction/two_phase_scraper.py",
        "class": "TwoPhaseContentScraper",
        "download_phase": [
          "Check cache.is_cached(book_id)",
          "If cached: return cache.load(book_id)",
          "If not: connector.navigate_to_book(), sleep(3), get page_source",
          "Save to cache, return content"
        ],
        "processing_phase": [
          "Parse HTML with BeautifulSoup(html, 'html.parser')",
          "Find transcript_div = soup.find('div', id='transcript-content')",
          "Get all surfaces = transcript_div.find_all('surface')",
          "For each requested page_number:",
          "  - Find surface with n=str(page_number)",
          "  - Extract text: for element in surface.descendants: if element.name is None: lines.append(element.strip())",
          "  - Generate image URL using _generate_image_url(page_number)",
          "  - Clean lines, detect verse numbers, create PageContent with image_url",
          "Return BookStorage with all pages"
        ],
        "image_url_generation": {
          "critical_importance": "Image URLs enable page-to-text alignment, verification, and academic citation",
          "opendigi_pattern": "OpenDigi uses IIIF Image API with consistent URL structure",
          "url_format": "https://opendigi.ub.uni-tuebingen.de/opendigi/image/{book_id}/{book_id}_{page:03d}.jp2/full/full/0/default.jpg",
          "implementation": [
            "Create method _generate_image_url(page_number: int) -> str",
            "Format: f'https://opendigi.ub.uni-tuebingen.de/opendigi/image/{self.book_id}/{self.book_id}_{page_number:03d}.jp2/full/full/0/default.jpg'",
            "Note: Page numbers are zero-padded to 3 digits (001, 002, 010, 150, etc.)",
            "Example: GaXXXIV5a page 7 → GaXXXIV5a_007.jp2/full/full/0/default.jpg"
          ],
          "verification": "Test URLs with curl -I to ensure HTTP 200 response",
          "page_content_field": "Include image_url in PageContent model and populate for every page"
        }
      },
      "step_7_implement_cli": {
        "file": "cli/commands.py",
        "framework": "Click for CLI, Rich for styled output",
        "extract_command": [
          "Arguments: url, --output, --formats, --start-page, --end-page, --headless",
          "Flow: BookIdentifier(url) → GundertPortalConnector → TwoPhaseContentScraper → BookStorage → save JSON",
          "Display statistics table with Rich"
        ]
      }
    },
    "testing_instructions": [
      "Test with: gundert-scraper extract https://opendigi.ub.uni-tuebingen.de/opendigi/GaXXXIV5a --start-page 7 --end-page 7",
      "Verify cache created in ./cache/GaXXXIV5a_content.json (~771KB)",
      "Run again with different page range - should use cache (instant)",
      "Check output JSON structure matches schema",
      "Verify image_url field is present in all pages",
      "Test image URL accessibility: curl -I <image_url> should return HTTP 200",
      "Confirm page number padding: page 7 → _007.jp2, page 150 → _150.jp2"
    ],
    "common_pitfalls": [
      "Not waiting long enough for SPA to load (use sleep 3-5 seconds)",
      "Trying to extract text with CSS selectors instead of parsing TEI structure",
      "Not preserving line breaks - use TreeWalker or descendants iteration",
      "Forgetting to extract image URLs for page-to-image mapping",
      "Incorrect image URL padding - must be zero-padded to 3 digits (007 not 7)",
      "Not validating image URLs - test with curl to ensure accessibility",
      "Hardcoding image URLs instead of generating them programmatically",
      "CRITICAL: Auto-deleting cache - Cache must NEVER be automatically deleted by any operation",
      "Not checking cache before download - Always check cache first to avoid redundant downloads",
      "Calling cache.clear() from extract/transform code - Only user commands should clear cache",
      "Not using force_redownload flag - Re-downloading should require explicit user intent",
      "Committing cache/ to git - Always add cache/ to .gitignore"
    ]
  },

  "recreate_from_scratch_checklist_old": [
    {
      "step": "Environment Setup",
      "actions": [
        "Create Python 3.10+ virtual environment",
        "Install Chrome/Chromium browser",
        "Set up pyproject.toml with exact dependencies",
        "Initialize git repository with proper .gitignore"
      ]
    },
    {
      "step": "Core Infrastructure",
      "actions": [
        "Implement modular directory structure",
        "Create Pydantic schemas (BookStorage, PageContent)",
        "Build caching system with RawContentCache",
        "Set up logging and configuration management"
      ]
    },
    {
      "step": "SPA Extraction Engine", 
      "actions": [
        "Implement DownloadPhase with Selenium integration",
        "Build ProcessingPhase with JavaScript execution",
        "Create TwoPhaseContentScraper orchestrator",
        "Add SPA detection and fallback mechanisms"
      ]
    },
    {
      "step": "Transformation Pipeline",
      "actions": [
        "Build BaseTransformer abstract class",
        "Implement TEI XML transformer (working reference)",
        "Create DOCX transformer with styling",
        "Debug and fix USFM transformer compatibility"
      ]
    },
    {
      "step": "CLI and Validation",
      "actions": [
        "Build Click-based CLI with rich console output", 
        "Implement extract/transform/validate commands",
        "Create format-specific validators",
        "Add batch processing capabilities"
      ]
    },
    {
      "step": "Quality Assurance",
      "actions": [
        "Write comprehensive test suite (target 88% coverage)",
        "Create integration tests with real URLs",
        "Add performance benchmarks",
        "Document all APIs and usage patterns"
      ]
    }
  ],

  "immediate_next_steps": [
    "Verify all format transformations with real extracted data", 
    "Complete comprehensive documentation (USER_GUIDE.md, DEVELOPER_GUIDE.md)",
    "Package for PyPI distribution with proper entry points",
    "Add comprehensive error handling and logging throughout system"
  ],
  
  "sample_books": [
	{
		"url": "https://opendigi.ub.uni-tuebingen.de/opendigi/GaXXXIV5a",
		"type": "Bible",
		"output": "usfm"
	},
	{
		"url": "https://opendigi.ub.uni-tuebingen.de/opendigi/CiXIV133",
		"type": "dictionary",
		"output": "tei"
	},
	{
		"url": "https://opendigi.ub.uni-tuebingen.de/opendigi/GaXXXIV2",
		"type": "book",
		"output": "tei, docx"
	}
  ],

  "reference_files": [
    "cache/GaXXXIV5a_transcript.html - downloaded content",
    "output/psalms_transformed/GaXXXIV5a.xml - Working TEI XML output (79KB Malayalam Psalms)",
    "src/gundert_portal_scraper/core/download_phase.py - SPA optimization implementation",
    "src/gundert_portal_scraper/core/processing_phase.py - JavaScript extraction engine", 
    "src/gundert_portal_scraper/transformations/plugins/tei_transformer.py - Working transformer reference",
    "README.md - Complete project documentation and architecture overview"
  ]
}